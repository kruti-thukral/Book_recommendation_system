# -*- coding: utf-8 -*-
"""user-based recommendation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Uzu2I_fmVjEi13X0VKt5Hz68s2bPk5lC
"""

# Importing required libraries
import sys
import pandas as pd
import os

# Reading the complete data into a pandas object
df=pd.read_csv('/content/drive/Shared drives/Project_256/Complete_Data.csv')

df.head()

df.info()

# Selecting the required columns
content_data = df[['user_id', 'book_id', 'country_code', 'language_code', 'authorid', 'rating', 'description', 'title']]

content_data.head()

# Filling the NaN values
content_data = content_data.fillna('')

# Dividing the data into training and testing data
import numpy as np
limit = np.random.rand(len(content_data)) < 0.7
training_data = content_data[limit]
test_data = content_data[~limit]

training_data

test_data

# Groupby on user-id to get all the data of one user in a single record
data = training_data.groupby(['user_id']).agg(lambda x: list(x)).reset_index()

data

data.info()

groupby_data=data

# After applying groupby function, we get a lot of data in a record
# Limiting the data to 1-3 entries
groupby_data['title'] = groupby_data['title'].apply(lambda x: x[:1])
groupby_data['authorid'] = groupby_data['authorid'].apply(lambda x: x[:3])
groupby_data['country_code'] = groupby_data['country_code'].apply(lambda x: x[:3])
groupby_data['language_code'] = groupby_data['language_code'].apply(lambda x: x[:3])
groupby_data['description'] = groupby_data['description'].apply(lambda x: x[:3])

groupby_data

# Function that creates a soup out of the desired metadata
def create_soup(x):
    return ' '.join(x['description']) + ' '.join(x['authorid']) + ' '.join(x['language_code']) + ' '.join(x['country_code'])

# Create the new soup feature
groupby_data['soup'] = groupby_data.apply(create_soup, axis=1)

groupby_data = groupby_data.applymap(str).reset_index()

# Importing TfidfVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer

#Define a TF-IDF Vectorizer Object. Remove all english stopwords
tf = TfidfVectorizer(stop_words='english')

#Construct the required TF-IDF matrix by applying the fit_transform method on the overview feature
count_matrix = count.fit_transform(groupby_data['soup'])

count_matrix.shape

# Importing cosine similarity
# Computing the cosine similarity score of one user against all the other users 
print("Computing Similarity...")
from sklearn.metrics.pairwise import cosine_similarity
cosine_similarities = cosine_similarity(count_matrix, count_matrix)
print("Similarity Computed!")

indices = pd.Series(groupby_data.index, index=groupby_data['user_id']).drop_duplicates()

def user_based_recommender(user_id, cosine_sim=cosine_similarities, df=groupby_data, indices=indices):
    
    idx = indices[user_id]

    # Get the pairwsie similarity scores of all users with that user
    # And convert it into a list of tuples
    sim_scores = list(enumerate(cosine_sim[idx]))

    # Sort the users based on the cosine similarity scores
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)

    # Get the scores of the 10 most similar users. Ignore the first one.
    sim_scores = sim_scores[1:11]

    # Get the user indices
    user_indices = [i[0] for i in sim_scores]

    book =[]
    score = []
    index=[]
    book = groupby_data['title'].iloc[user_indices]
    index,score = map(list,zip(*sim_scores))
    recommender_metadata=pd.DataFrame({'Books' : book, 'Score' : score})
    #print(recommender_metadata)
    #score = pd.DataFrame(score)
    #score=pd.DataFrame({'Score' : score,'Probability' : list2})
    #score['val'] = score
    #score.set_index('val')
    #yes['title'] = yes
    #yes['score'] = score
    recommender_metadata.to_csv("user_based_recommendations.csv", index=False)

# Get Book Recommendations for a particular user by providing the user_id
user_based_recommender('0006260f85929db85eddee3a0bd0e504')

# Evaluation: Check if the recommended books or similar books are present in the test data
test_data[test_data['user_id'].astype(str).str.contains('0006260f85929db85eddee3a0bd0e504')]