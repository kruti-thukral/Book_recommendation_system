# -*- coding: utf-8 -*-
"""Copy of Data_Preprocessing_new.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yfsFXXJyHkCTBO6al1mNGev4IVClBRik
"""

pip install gdown

# Importing required libraries
import sys
import pandas as pd
import gdown
import os

# Downloading required datasets from goodreads
url1 = 'https://drive.google.com/uc?id=1CCj-cQw_mJLMdvF_YYfQ7ibKA-dC_GA2'
url2 = 'https://drive.google.com/uc?id=1V4MLeoEiPQdocCbUHjR_7L9ZmxTufPFe'
url3='https://drive.google.com/uc?id=1ICk5x0HXvXDp5Zt54CKPh5qz1HyUIn9m'
gdown.download(url1, output=None, quiet=False)
gdown.download(url2, output=None, quiet=False)
gdown.download(url3, output=None, quiet=False)

# Converting json to string to pandas object
# Reading the goodreads book dataset
fn = filename
books = pd.read_json(fn, lines=True, compression='gzip')
books.tail()

books.info()

books.describe()

# Display duplicated rows
display(books[books.duplicated(['book_id'], keep=False)])

books_20 = books.head(20)

# Visualization
# Graph: Average Ratings per book 
import numpy as np
import matplotlib.pyplot as plt
books_20.plot(kind='bar',x='book_id',y='average_rating',color='blue')
plt.show()

# Scatter plot for number of reviews per book
books.plot(kind='scatter',x='book_id',y='text_reviews_count',color = 'blue')
plt.show()

"""###### As part of pre-processing, we will drop columns that we do not require . After pre-processing, the book data will have below features
text_reviews_count      
country_code            
language_code           
popular_shelves         
is_ebook                
average_rating   
description            
authors                 
publication_year        
book_id                 
ratings_count           
title                   
title_without_series
"""

# Reading the goodreads (user-book) interactions dataset
fn1 = filename
interactions = pd.read_json(fn1, lines=True, compression='gzip')
interactions.tail()

interactions.info()

interactions.review_id.nunique()

display(interactions[interactions.duplicated(['user_id', 'book_id'], keep=False)])

# Preprocessing for visualization
shelve_user = interactions['user_id'].value_counts().value_counts().reset_index().sort_values('index').values
read_user = interactions['user_id'].loc[interactions['is_read']>0].value_counts().value_counts().reset_index().sort_values('index').values
rate_user = interactions['user_id'].loc[interactions['rating']>0].value_counts().value_counts().reset_index().sort_values('index').values
#review_user = df['user_id'].loc[df['is_reviewed']>0].value_counts().value_counts().reset_index().sort_values('index').values

shelve_book = interactions['book_id'].value_counts().value_counts().reset_index().sort_values('index').values
read_book = interactions['book_id'].loc[interactions['is_read']>0].value_counts().value_counts().reset_index().sort_values('index').values
rate_book = interactions['book_id'].loc[interactions['rating']>0].value_counts().value_counts().reset_index().sort_values('index').values

# Log-log plot of the distribution of users
import matplotlib.pyplot as plt
plt.loglog(shelve_user[:,0], shelve_user[:,1], label='shelve')
plt.loglog(read_user[:,0], read_user[:,1], label='read')
plt.loglog(rate_user[:,0], rate_user[:,1], label='rate')
#plt.loglog(review_user[:,0], review_user[:,1], label='review')
plt.xlabel('rank')
plt.ylabel('frequency')
plt.title('Log-Log Plot of the Distribution of Users')
plt.legend(loc='upper right')

# Scatter plot for ratings of books
interactions_10000 = interactions.head(10000)
interactions_10000.plot(kind='scatter',x='book_id',y='rating',color = 'blue')
plt.show()

# Reading the goodreads reviews data into a pandas object
fn2 = '/content/drive/Shared drives/Project_256/goodreads_reviews_comics_graphic.json.gz'
reviews = pd.read_json(fn2, lines=True, compression='gzip')
reviews.tail()

reviews.info()

# Selecting the required columns from the books dataset
books_filtered=books[['text_reviews_count','country_code','language_code','popular_shelves','is_ebook','average_rating','description','authors','publication_year','book_id','ratings_count','title','title_without_series']]

books_filtered.head()

# Selecting required columns from the interactions dataset
interactions_filtered = interactions[['user_id','book_id','review_id','is_read','rating','date_added','date_updated','read_at','started_at']]

interactions_filtered.head()

# Selecting required columns from the reviews dataset
reviews_filtered = reviews[['review_id','review_text','n_votes','n_comments']]

reviews_filtered.head()

len(reviews_filtered)

len(interactions_filtered)

# Merging the books and interactions dataset based on book_id
books_interactions_merged_data = pd.merge(books_filtered, interactions_filtered, on='book_id')

len(books_interactions_merged_data)

# Merging the books-interactions merged data with the reviews dataset based on review_id
data_merged = pd.merge(books_interactions_merged_data, reviews_filtered, on='review_id', how='left')

data_merged

len(data_merged)

data_merged.info()

data_merged.isnull().sum()

#data_merged['authors'][0][0]

authors=data_merged['authors']

print(authors.head(100))

authors.tail()

# Extracting author_id from authors column
authors2=[]
for i in authors:
    author=[]
    for j in i:
      author.append(j['author_id'])
    #print(author)
    authors2.append(author)

#authors2

#authorID = pd.DataFrame(authors2)

#authorID.head()

data_merged2=data_merged

# Merging the author_id column in the final merged data
data_merged2['authorid'] = authors2
# Final merged data created

# Creating a correlation map on the final merged data
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.pylab as pylab
import seaborn as sns

def plot_correlation_map( df ):
    corr = data_merged_less.corr()
    _ , ax = plt.subplots( figsize =( 12 , 10 ) )
    cmap = sns.diverging_palette( 220 , 10 , as_cmap = True )
    _ = sns.heatmap(
        corr, 
        cmap = cmap,
        square=True, 
        cbar_kws={ 'shrink' : .9 }, 
        ax=ax, 
        annot = True, 
        annot_kws = { 'fontsize' : 12 }
    )

plot_correlation_map(data_merged_less)
plt.savefig('corr_map.png')

len(data_merged2)

# Writing the pandas object to a csv file
data_merged2.to_csv('/content/drive/Shared drives/Project_256/Complete_Data.gz', compression='gzip')